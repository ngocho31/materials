%\chapter{Tài li?u tham kh?o}
%\addcontentsline{toc}{chapter}{Tài li?u tham kh?o}
%\setheader{Tài li?u tham kh?o}
%\label{sec:referemce}

%% We use the 'etaremune' environment (the reverse of 'enumerate') to get a
%% numbered list of publications in reverse chronological order. If the list of
%% authors is long, it might be useful to emphasize your own name with \textbf.

\chapter{Tài liệu tham khảo}
\begin{enumerate}
%chp01
\item \label{refer:1} McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in nervous activity. Bull. Math. Bioph. 5, 115–133 (1943)
\item \label{refer:2} Hebb, D.: The Organization of Behavior. Wiley, New York (1949)
\item \label{refer:3} Rosenblatt, F.: The perceptron: a probabilistic model for information storage and organization in the brain. Psychol. Rev. 65(6), 386–408 (1958)
\item \label{refer:4} Rosenblatt, F.: Principles of Neurodynamics. Spartan Books, Washington (1962)
\item \label{refer:5} Widrow, B., Hoff, M.: Adaptive switching circuits. Technical report 1553-1, Stanford Electron. Labs., Stanford, June 1960
\item \label{refer:6} Hopfield,J.J.:Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. USA 79, 2554–2558 (1982)
%chp04
\item \label{refer:7} Tiago Duque, An Introduction to NLP — explanation and examples.
\item \label{refer:8} Christopher Marshall, What is named entity recognition (NER) and how can I use it?
\item \label{refer:9} Arun Jagota, Topic Modeling In NLP
\item \label{refer:10} Tiago Duque, NLP Preprocessing Pipeline — what, when, why?
\item \label{refer:11} Gergely D. Németh, Machine Translation: A Short Overview
\item \label{refer:12} Ahmed Menshawy. \textit{Deep Learning By Example}. Packt Publishing, 2018.
\item \label{refer:13} \href{https://nguyentruonglong.net/mo-hinh-cbow-continuous-bag-of-words.html}{Mô hình CBOW - Nguyễn Trương Long}
\item \label{refer:14} \href{https://pixta.vn/tim-moi-lien-he-giua-cac-tag-voi-nhau-bang-word-embedding/}{Tìm mối liên hệ giữa các tag với nhau bằng Word Embedding}
%chp05
\item \label{refer:15} \href{https://en.wikipedia.org/wiki/Convolutional_neural_network}{Convolutional neural network. (2020, July 29). Retrieved August 01, 2020}
\item \label{refer:16} Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4), 193-202. doi:10.1007/ bf00344251
\item \label{refer:17} \href{https://medium.com/datadriveninvestor/convolutional-neural-network-cnn-simplified-ecafd4ee52c5}{Khandelwal, R. (2018, October 18). Convolutional Neural Network (CNN) Simplified. Retrieved August 01, 2020}
%chp06
\item \label{refer:18} \href{http://deeplearning.net/tutorial/lenet.html}{Convolutional Neural Networks (LeNet)}
\item \label{refer:19} \href{https://github.com/pprzetacznik/nlp-n-grams}{pprzetacznik , Simple Implement Ngram}

\item \label{refer:20} Daniël H: https://medium.com/machine-learning-at-petiteprogrammer/\\sampling-strategies-for-recurrent-neural-networks-9aea02a6616f
\item \label{refer:21} Numpy document: https://docs.scipy.org/doc//numpy-1.10.4/reference/\\generated/numpy.random.choice.html
\item \label{refer:22} Shervine A: https://stanford.edu/~shervine/teaching/cs-230/\\cheatsheet-recurrent-neural-networks
\item \label{refer:23} Priya D:
https://towardsdatascience.com/train-a-gpt-2-transformer-to-write-harry-potter-books-edf8b2e3f3db

\item \label{refer:24} \textit{Levenshtein distance}, URL: https://en.wikipedia.org/wiki/Levenshtein\_distance
\item \label{refer:25} Peter Norvig, \textit{How to Write a Spelling Corrector}, URL: http://norvig.com/spell-correct.html
\end{enumerate}